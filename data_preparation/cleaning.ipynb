{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ba7edb",
   "metadata": {},
   "source": [
    "## Using Lemmatization with Zemberek-Python\n",
    "Instead of using a static suffix list, we will use `zemberek-python` for lemmatization. This allows for more accurate matching of keywords by finding their root forms, handling Turkish morphology correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de205521",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "804ac793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temizlik kodu\n",
    "#Bu kod wpdeki excel alıp temizler, sütunları düzeltir küfürleri ve ingilizce ve saçma kelimeler içeren satırları siler.\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "#from langdetect import detect\n",
    "import json\n",
    "from zemberek import (\n",
    "    TurkishMorphology,\n",
    ")\n",
    "from openpyxl.styles import PatternFill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f52bec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:18:06,703 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 5.869441032409668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dosya yolları\n",
    "INPUT_PATH = \"../dataset/reviews.csv\"\n",
    "OUTPUT_PATH = \"../dataset/cleaned_reviews_zemberek.xlsx\"\n",
    "\n",
    "# KEYWORDS\n",
    "# SWEAR_WORDS_PATH = \"./constants/swear_words.json\" # No longer needed directly if included in sentiment\n",
    "ASPECTS_KEYWORDS_PATH = \"./constants/aspects_keywords.json\"\n",
    "SENTIMENT_KEYWORDS_PATH = \"./constants/sentiment_keywords.json\"\n",
    "COLORS_MAP_PATH = \"./constants/colors_map.py\"\n",
    "\n",
    "# Initialize Zemberek Morphology\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# Keyword lists will be loaded from JSON\n",
    "ASPECTS_KEYWORDS = {}\n",
    "SENTIMENT_KEYWORDS = {}\n",
    "\n",
    "\n",
    "\n",
    "COLOR_MAP = {\n",
    "    \"Olumlu\": PatternFill(start_color=\"90EE90\", fill_type=\"solid\"),\n",
    "    \"Olumsuz\": PatternFill(start_color=\"FFA07A\", fill_type=\"solid\"),\n",
    "    \"Nötr\": PatternFill(start_color=\"D3D3D3\", fill_type=\"solid\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4ebc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file\n",
    "def read_json(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        # read the json file\n",
    "        data = json.load(f)\n",
    "        # close the file\n",
    "\n",
    "        f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2c42625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspects keywords count:  631\n",
      "Sentiment keywords count:  919\n"
     ]
    }
   ],
   "source": [
    "# GET THE ASPECTS KEYWORDS FROM THE ASPECTS KEYWORDS FILE\n",
    "aspects_data = read_json(ASPECTS_KEYWORDS_PATH)\n",
    "ASPECTS_KEYWORDS = aspects_data[\"ASPECTS_KEYWORDS\"]\n",
    "\n",
    "# GET THE SENTIMENT KEYWORDS FROM THE SENTIMENT KEYWORDS FILE\n",
    "sentiments_data = read_json(SENTIMENT_KEYWORDS_PATH)\n",
    "SENTIMENT_KEYWORDS = sentiments_data[\"SENTIMENT_KEYWORDS\"]\n",
    "\n",
    "# print the aspects keywords\n",
    "# Calculate the total number of keywords across all aspect categories\n",
    "total_aspect_keywords = sum(len(keywords) for keywords in ASPECTS_KEYWORDS.values())\n",
    "print(\"Aspects keywords count: \", total_aspect_keywords)\n",
    "# print the sentiment keywords\n",
    "print(\"Sentiment keywords count: \", len(SENTIMENT_KEYWORDS[\"Olumlu\"]) + len(SENTIMENT_KEYWORDS[\"Olumsuz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79427a4d",
   "metadata": {},
   "source": [
    "### TEXT PROCESSING (with Zemberek):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13e83953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any given text to string and lowercase\n",
    "def to_str_lowercase(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "# Replace Turkish characters with English equivalents for normalization after lemmatization\n",
    "def normalize_text(text):\n",
    "    replacements = str.maketrans(\"ÇĞİÖŞÜçğıöşü\", \"CGIOSUcgiosu\")\n",
    "    return text.translate(replacements)\n",
    "\n",
    "# Normalize a single keyword (used for keyword lists before lemmatization)\n",
    "def normalize_keyword_simple(keyword):\n",
    "    keyword = to_str_lowercase(keyword)\n",
    "    keyword = normalize_text(keyword)\n",
    "    return keyword\n",
    "\n",
    "# Remove extra spaces from the text\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# --- ZEMBEREK BASED LEMMATIZATION AND MATCHING ---\n",
    "# Function to get lemma of a word using Zemberek\n",
    "def get_lemma(word):\n",
    "    try:\n",
    "        results = morphology.analyze(word)\n",
    "        # Check if results is a non-empty list\n",
    "        if results and isinstance(results, list) and len(results) > 0:\n",
    "            # Access the first analysis result\n",
    "            first_analysis = results[0]\n",
    "            # Check if the necessary attributes exist\n",
    "            if hasattr(first_analysis, 'item') and hasattr(first_analysis.item, 'lemma'):\n",
    "                return first_analysis.item.lemma\n",
    "            else:\n",
    "                # Log or handle unexpected structure if needed\n",
    "                # print(f\"Warning: Unexpected analysis structure for word '{word}': {first_analysis}\")\n",
    "                pass # Fall through to return original word\n",
    "        # Handle cases where results is not a list or is empty\n",
    "        # print(f\"Warning: No valid analysis found for word '{word}'. Results: {results}\")\n",
    "        pass # Fall through to return original word\n",
    "    except Exception as e:\n",
    "        # Log unexpected errors during analysis if needed\n",
    "        # print(f\"Error analyzing word '{word}': {e}\")\n",
    "        pass # Fall through to return original word\n",
    "\n",
    "    # Return original word if analysis fails or encounters issues\n",
    "    return word\n",
    "\n",
    "# Check if a normalized keyword's lemma exists in the lemmatized text\n",
    "def has_lemmatized_keyword(text_lemmatized_normalized_set, normalized_keyword_lemma):\n",
    "    # Check if the already normalized keyword lemma exists in the set of lemmatized words from the text\n",
    "    return normalized_keyword_lemma in text_lemmatized_normalized_set\n",
    "\n",
    "# Pre-lemmatize and normalize a text\n",
    "def lemmatize_normalize_text(text):\n",
    "    text = to_str_lowercase(text) # Lowercase the input text\n",
    "    # Tokenize text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    # Lemmatize each word in the text and normalize (English chars)\n",
    "    lemmatized_normalized_words = {normalize_text(get_lemma(word)) for word in words}\n",
    "    return lemmatized_normalized_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07892c53",
   "metadata": {},
   "source": [
    "### PROCESS CONSTANTS (with Zemberek Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f422f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized & Normalized aspects keywords loaded: Yes\n",
      "Lemmatized & Normalized sentiment keywords loaded: Yes\n",
      "Sample lemmatized 'Grafik' keywords:  ['2d', 'cizimler', 'gorunum', 'hareket animasyonu', 'gorsel stil']\n",
      "Sample lemmatized 'Olumlu' keywords:  ['akil almaz', 'erdemli', 'kacirmayin', 'devrim niteliginde', 'optimize edilmis']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize and normalize all keywords in the dictionaries\n",
    "def lemmatize_normalize_keywords(keywords_dict):\n",
    "    processed_dict = {}\n",
    "    for category, keywords in keywords_dict.items():\n",
    "        # Lemmatize and normalize keywords to lowercase and English chars\n",
    "        # Ensure keywords are strings before processing\n",
    "        lemmatized_normalized_keywords = {normalize_text(get_lemma(str(k).lower())) for k in keywords}\n",
    "        processed_dict[category] = lemmatized_normalized_keywords # Use set for faster lookups\n",
    "    return processed_dict\n",
    "\n",
    "# Lemmatize and normalize keywords once at the start\n",
    "LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS = lemmatize_normalize_keywords(ASPECTS_KEYWORDS)\n",
    "LEMMATIZED_NORMALIZED_SENTIMENT_KEYWORDS = lemmatize_normalize_keywords(SENTIMENT_KEYWORDS)\n",
    "\n",
    "print(\"Lemmatized & Normalized aspects keywords loaded: Yes\")\n",
    "print(\"Lemmatized & Normalized sentiment keywords loaded: Yes\")\n",
    "\n",
    "# Example: Print some lemmatized keywords to verify\n",
    "if 'Grafik' in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS:\n",
    "    print(\"Sample lemmatized 'Grafik' keywords: \", list(LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS['Grafik'])[:5])\n",
    "if 'Olumlu' in LEMMATIZED_NORMALIZED_SENTIMENT_KEYWORDS:\n",
    "    print(\"Sample lemmatized 'Olumlu' keywords: \", list(LEMMATIZED_NORMALIZED_SENTIMENT_KEYWORDS['Olumlu'])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword match function - NOW USES LEMMATIZATION\n",
    "def keyword_match(text_lemmatized_normalized_set, normalized_keyword_lemma):\n",
    "    # Uses the new function which checks lemmatized forms\n",
    "    return has_lemmatized_keyword(text_lemmatized_normalized_set, normalized_keyword_lemma)\n",
    "\n",
    "# ---- MAIN ANALYSIS FUNCTION (Updated with Zemberek & Sentence-Level Sentiment) ----\n",
    "# Define common Turkish negation word lemmas (adjust if Zemberek produces different lemmas)\n",
    "NEGATION_LEMMAS = {\"değil\", \"yok\", \"hiç\"}\n",
    "\n",
    "def analyze_review(text):\n",
    "    # Initialize result with Nötr for all aspects\n",
    "    result = {aspect: \"Nötr\" for aspect in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS}\n",
    "    \n",
    "    # Ensure text is a string before processing\n",
    "    if pd.isna(text):\n",
    "        return result # Return default neutral if input is NaN\n",
    "    text = str(text)\n",
    "\n",
    "    # Lemmatize and normalize the entire review text once for efficiency\n",
    "    review_lemmatized_normalized_set = lemmatize_normalize_text(text)\n",
    "    \n",
    "    found_aspects_with_keywords = {} # Store aspect and the specific keyword lemma found\n",
    "    \n",
    "    # First pass: Identify all aspects mentioned\n",
    "    for aspect, lemmatized_keywords_set in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS.items():\n",
    "        matching_keyword_lemmas = review_lemmatized_normalized_set.intersection(lemmatized_keywords_set)\n",
    "        if matching_keyword_lemmas:\n",
    "            found_aspects_with_keywords[aspect] = list(matching_keyword_lemmas) # Store matched lemmas if needed later\n",
    "    \n",
    "    # Second pass: Determine sentiment for found aspects based on relevant sentences\n",
    "    if found_aspects_with_keywords:\n",
    "        sentences = re.split(r'[.!?]+', text.lower()) # Split original text into sentences, handle multiple delimiters\n",
    "        pos_keywords_lemmas = LEMMATIZED_NORMALIZED_SENTIMENT_KEYWORDS.get(\"Olumlu\", set())\n",
    "        neg_keywords_lemmas = LEMMATIZED_NORMALIZED_SENTIMENT_KEYWORDS.get(\"Olumsuz\", set())\n",
    "\n",
    "        for aspect in found_aspects_with_keywords: # Iterate through aspects found in the review\n",
    "            aspect_sentiment_score = 0\n",
    "            aspect_keywords_set = LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS[aspect] # Get all keyword lemmas for the aspect\n",
    "\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence: continue # Skip empty sentences\n",
    "\n",
    "                sentence_lemmatized_normalized_set = lemmatize_normalize_text(sentence)\n",
    "\n",
    "                # Check if the sentence contains any keyword lemma for the current aspect\n",
    "                if not aspect_keywords_set.isdisjoint(sentence_lemmatized_normalized_set):\n",
    "                    # Sentence is relevant to the aspect, now analyze its sentiment\n",
    "                    positive_matches = sentence_lemmatized_normalized_set.intersection(pos_keywords_lemmas)\n",
    "                    negative_matches = sentence_lemmatized_normalized_set.intersection(neg_keywords_lemmas)\n",
    "                    \n",
    "                    sentence_score = len(positive_matches) - len(negative_matches)\n",
    "\n",
    "                    # Basic Negation Check: If a negation lemma exists in the sentence, neutralize or flip the score?\n",
    "                    # Simple approach: If negation present, reduce confidence by neutralizing score for this sentence.\n",
    "                    # More complex logic could check proximity of negation to sentiment words.\n",
    "                    if not NEGATION_LEMMAS.isdisjoint(sentence_lemmatized_normalized_set):\n",
    "                        # If negation detected, perhaps ignore sentiment score for this sentence or reduce its weight\n",
    "                        # Let's try neutralizing the score for this sentence if negation is found\n",
    "                        sentence_score = 0 \n",
    "                        # Alternatively, could flip: sentence_score = -sentence_score \n",
    "                         # Or reduce magnitude: sentence_score *= 0.5 \n",
    "\n",
    "                    aspect_sentiment_score += sentence_score\n",
    "\n",
    "            # Determine final sentiment for the aspect based on aggregated score\n",
    "            if aspect_sentiment_score > 0:\n",
    "                result[aspect] = \"Olumlu\"\n",
    "            elif aspect_sentiment_score < 0:\n",
    "                result[aspect] = \"Olumsuz\"\n",
    "            # else: remains \"Nötr\" (if score is 0)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d59bcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VERİ YÜKLE ---\n",
    "df = pd.read_csv(INPUT_PATH, usecols=[\"app_id\", \"review_text\"], dtype={\"app_id\": str, \"review_text\": str}).dropna(subset=['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a7ef103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame head:\n",
      "    app_id                                        review_text\n",
      "0  1245620  İlk 40 saatimde nereye gitmem gerektiğini ne y...\n",
      "1  1245620  Bu oyunda Malenia bossunu tasarlıyan arkadaşa ...\n",
      "2  1245620  Güzel oyun  atmosfer kontroller vs. güzel ama ...\n",
      "3  1245620  Oyunun devasa bir haritası var açık dünya olma...\n",
      "4  1245620       oynu bitirdiğinizde huzurluca ölebilirisiniz\n",
      "Total reviews loaded: 7245\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows of the loaded data\n",
    "print(\"Original DataFrame head:\")\n",
    "print(df.head())\n",
    "print(f\"Total reviews loaded: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ebb2142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis on the full DataFrame...\n",
      "Processing 7245 reviews...\n",
      "  Processed 500/7245 reviews...\n",
      "  Processed 500/7245 reviews...\n",
      "  Processed 1000/7245 reviews...\n",
      "  Processed 1000/7245 reviews...\n",
      "  Processed 1500/7245 reviews...\n",
      "  Processed 1500/7245 reviews...\n",
      "  Processed 2000/7245 reviews...\n",
      "  Processed 2000/7245 reviews...\n",
      "  Processed 2500/7245 reviews...\n",
      "  Processed 2500/7245 reviews...\n",
      "  Processed 3000/7245 reviews...\n",
      "  Processed 3000/7245 reviews...\n",
      "  Processed 3500/7245 reviews...\n",
      "  Processed 3500/7245 reviews...\n",
      "  Processed 4000/7245 reviews...\n",
      "  Processed 4000/7245 reviews...\n",
      "  Processed 4500/7245 reviews...\n",
      "  Processed 4500/7245 reviews...\n",
      "  Processed 5000/7245 reviews...\n",
      "  Processed 5000/7245 reviews...\n",
      "  Processed 5500/7245 reviews...\n",
      "  Processed 5500/7245 reviews...\n",
      "  Processed 6000/7245 reviews...\n",
      "  Processed 6000/7245 reviews...\n",
      "  Processed 6500/7245 reviews...\n",
      "  Processed 6500/7245 reviews...\n",
      "  Processed 7000/7245 reviews...\n",
      "  Processed 7000/7245 reviews...\n",
      "Analysis complete. Processed 7245 reviews. Results DataFrame head:\n",
      "    app_id                                        review_text  Grafik      AI  \\\n",
      "0  1245620  İlk 40 saatimde nereye gitmem gerektiğini ne y...    Nötr    Nötr   \n",
      "1  1245620  Bu oyunda Malenia bossunu tasarlıyan arkadaşa ...    Nötr    Nötr   \n",
      "2  1245620  Güzel oyun  atmosfer kontroller vs. güzel ama ...  Olumlu    Nötr   \n",
      "3  1245620  Oyunun devasa bir haritası var açık dünya olma...    Nötr  Olumlu   \n",
      "4  1245620       oynu bitirdiğinizde huzurluca ölebilirisiniz    Nötr    Nötr   \n",
      "\n",
      "   Oynanis Ses ve Muzik Oyun Dunyasi Topluluk ve Sosyal Hikaye ve Senaryo  \\\n",
      "0  Olumsuz         Nötr         Nötr               Nötr              Nötr   \n",
      "1   Olumlu         Nötr         Nötr               Nötr              Nötr   \n",
      "2   Olumlu         Nötr       Olumlu               Nötr              Nötr   \n",
      "3   Olumlu         Nötr       Olumlu               Nötr              Nötr   \n",
      "4     Nötr         Nötr         Nötr               Nötr              Nötr   \n",
      "\n",
      "  Performans ve Teknik  \n",
      "0                 Nötr  \n",
      "1                 Nötr  \n",
      "2                 Nötr  \n",
      "3                 Nötr  \n",
      "4                 Nötr  \n",
      "Analysis complete. Processed 7245 reviews. Results DataFrame head:\n",
      "    app_id                                        review_text  Grafik      AI  \\\n",
      "0  1245620  İlk 40 saatimde nereye gitmem gerektiğini ne y...    Nötr    Nötr   \n",
      "1  1245620  Bu oyunda Malenia bossunu tasarlıyan arkadaşa ...    Nötr    Nötr   \n",
      "2  1245620  Güzel oyun  atmosfer kontroller vs. güzel ama ...  Olumlu    Nötr   \n",
      "3  1245620  Oyunun devasa bir haritası var açık dünya olma...    Nötr  Olumlu   \n",
      "4  1245620       oynu bitirdiğinizde huzurluca ölebilirisiniz    Nötr    Nötr   \n",
      "\n",
      "   Oynanis Ses ve Muzik Oyun Dunyasi Topluluk ve Sosyal Hikaye ve Senaryo  \\\n",
      "0  Olumsuz         Nötr         Nötr               Nötr              Nötr   \n",
      "1   Olumlu         Nötr         Nötr               Nötr              Nötr   \n",
      "2   Olumlu         Nötr       Olumlu               Nötr              Nötr   \n",
      "3   Olumlu         Nötr       Olumlu               Nötr              Nötr   \n",
      "4     Nötr         Nötr         Nötr               Nötr              Nötr   \n",
      "\n",
      "  Performans ve Teknik  \n",
      "0                 Nötr  \n",
      "1                 Nötr  \n",
      "2                 Nötr  \n",
      "3                 Nötr  \n",
      "4                 Nötr  \n"
     ]
    }
   ],
   "source": [
    "# --- ANALİZ (Using Zemberek) ---\n",
    "print(\"Starting analysis on the full DataFrame...\") # Modified print statement\n",
    "results = []\n",
    "# Use apply for potentially faster processing, especially on larger dataframes\n",
    "# analysis_results = df['review_text'].apply(analyze_review) # Use df here if using apply\n",
    "\n",
    "# Or iterate for easier debugging:\n",
    "total_rows = len(df) # Use df here\n",
    "print(f\"Processing {total_rows} reviews...\")\n",
    "for index, row in df.iterrows(): # Use df here instead of test_df\n",
    "    if pd.isna(row[\"review_text\"]):\n",
    "        print(f\"Skipping row {index} due to NaN review_text\")\n",
    "        analysis = {aspect: \"Nötr\" for aspect in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS}\n",
    "    else:\n",
    "        analysis = analyze_review(row[\"review_text\"])\n",
    "    results.append({\n",
    "        \"app_id\": row[\"app_id\"],\n",
    "        \"review_text\": row[\"review_text\"],\n",
    "        **analysis\n",
    "    })\n",
    "    # Print progress less frequently for the full dataset\n",
    "    if (index + 1) % 500 == 0: # Adjusted print frequency\n",
    "        print(f\"  Processed {index + 1}/{total_rows} reviews...\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"Analysis complete. Processed {len(df_results)} reviews. Results DataFrame head:\") # Modified print statement\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ef5da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to ../dataset/cleaned_reviews_zemberek.xlsx...\n",
      "Applying colors to Excel sheet...\n",
      "  Coloring column: Grafik (C)\n",
      "  Coloring column: AI (D)\n",
      "  Coloring column: Oynanis (E)\n",
      "Applying colors to Excel sheet...\n",
      "  Coloring column: Grafik (C)\n",
      "  Coloring column: AI (D)\n",
      "  Coloring column: Oynanis (E)\n",
      "  Coloring column: Ses ve Muzik (F)\n",
      "  Coloring column: Oyun Dunyasi (G)\n",
      "  Coloring column: Topluluk ve Sosyal (H)\n",
      "  Coloring column: Ses ve Muzik (F)\n",
      "  Coloring column: Oyun Dunyasi (G)\n",
      "  Coloring column: Topluluk ve Sosyal (H)\n",
      "  Coloring column: Hikaye ve Senaryo (I)\n",
      "  Coloring column: Performans ve Teknik (J)\n",
      "Adjusting column widths...\n",
      "  Coloring column: Hikaye ve Senaryo (I)\n",
      "  Coloring column: Performans ve Teknik (J)\n",
      "Adjusting column widths...\n",
      "Creating summary sheet...\n",
      "Creating summary sheet...\n",
      "Finding most common words in neutral reviews...\n",
      "Finding most common words in neutral reviews...\n",
      "Process Done✅: ../dataset/cleaned_reviews_zemberek.xlsx\n",
      "Process Done✅: ../dataset/cleaned_reviews_zemberek.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- RENKLİ KAYDET (Updated for Zemberek) ---\n",
    "print(f\"Saving results to {OUTPUT_PATH}...\")\n",
    "with pd.ExcelWriter(OUTPUT_PATH, engine=\"openpyxl\") as writer:\n",
    "    df_results.to_excel(writer, index=False, sheet_name=\"Analizler\")\n",
    "    worksheet = writer.sheets[\"Analizler\"]\n",
    "    \n",
    "    # Find aspect columns dynamically based on the processed keywords\n",
    "    aspect_columns = [col for col in df_results.columns if col in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS]\n",
    "    aspect_col_indices = {col: df_results.columns.get_loc(col) + 1 for col in aspect_columns} # +1 for 1-based index\n",
    "\n",
    "    # Apply coloring to aspect columns\n",
    "    print(\"Applying colors to Excel sheet...\")\n",
    "    for aspect, col_idx in aspect_col_indices.items():\n",
    "        col_letter = get_column_letter(col_idx)\n",
    "        print(f\"  Coloring column: {aspect} ({col_letter})\") # DEBUG PRINT\n",
    "        for row in range(2, len(df_results) + 2): # +2 because Excel is 1-based and header is row 1\n",
    "            cell = worksheet[f\"{col_letter}{row}\"]\n",
    "            sentiment_value = cell.value\n",
    "            # print(f\"    Row {row}, Cell {col_letter}{row}, Value: '{sentiment_value}'\") # Optional detailed DEBUG\n",
    "            if sentiment_value in COLOR_MAP:\n",
    "                # print(f\"      Applying color for: {sentiment_value}\") # Optional detailed DEBUG\n",
    "                cell.fill = COLOR_MAP[sentiment_value]\n",
    "            else:\n",
    "                # print(f\"      Applying default color (Nötr) for value: {sentiment_value}\") # Optional detailed DEBUG\n",
    "                cell.fill = COLOR_MAP[\"Nötr\"] # Default fill\n",
    "    \n",
    "    # Adjust column widths (optional)\n",
    "    print(\"Adjusting column widths...\")\n",
    "    worksheet.column_dimensions['A'].width = 15 # app_id\n",
    "    worksheet.column_dimensions['B'].width = 80 # review_text\n",
    "    for aspect, col_idx in aspect_col_indices.items():\n",
    "        worksheet.column_dimensions[get_column_letter(col_idx)].width = 20 # Aspect columns\n",
    "\n",
    "# --- ÖZET SHEET (Updated for Zemberek) ---\n",
    "print(\"Creating summary sheet...\")\n",
    "summary_data = []\n",
    "for aspect in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS: # Use keys from lemmatized dict\n",
    "    if aspect in df_results.columns:\n",
    "        counts = df_results[aspect].value_counts().to_dict()\n",
    "        summary_data.append({\n",
    "            \"Kategori\": aspect,\n",
    "            \"Olumlu\": counts.get(\"Olumlu\", 0),\n",
    "            \"Olumsuz\": counts.get(\"Olumsuz\", 0),\n",
    "            \"Nötr\": counts.get(\"Nötr\", 0)\n",
    "        })\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "with pd.ExcelWriter(OUTPUT_PATH, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "    df_summary.to_excel(writer, index=False, sheet_name=\"Özet\")\n",
    "\n",
    "# --- NÖTR KELİMELER (Updated for Zemberek) ---\n",
    "print(\"Finding most common words in neutral reviews...\")\n",
    "# Dynamically create the filter condition for all aspects being Nötr\n",
    "neutral_condition = None\n",
    "for aspect in LEMMATIZED_NORMALIZED_ASPECTS_KEYWORDS:\n",
    "    if aspect in df_results.columns:\n",
    "        condition = (df_results[aspect] == \"Nötr\")\n",
    "        if neutral_condition is None:\n",
    "            neutral_condition = condition\n",
    "        else:\n",
    "            neutral_condition &= condition\n",
    "\n",
    "if neutral_condition is not None and not df_results[neutral_condition].empty:\n",
    "    neutral_reviews = df_results[neutral_condition][\"review_text\"].tolist()\n",
    "    \n",
    "    neutral_corpus = \" \".join(map(str, neutral_reviews)).lower()\n",
    "    # Lemmatize and normalize words before counting\n",
    "    neutral_words_lemmatized_normalized = lemmatize_normalize_text(neutral_corpus)\n",
    "    # Filter out short words if needed (already handled by regex in lemmatize_normalize_text)\n",
    "    # neutral_words_filtered = [word for word in neutral_words_lemmatized_normalized if len(word) >= 3]\n",
    "    \n",
    "    # Count the frequency of lemmatized words\n",
    "    # Need to re-tokenize and lemmatize for counting, as the set lost frequency info\n",
    "    words_for_counting = re.findall(r'\\b\\w+\\b', neutral_corpus)\n",
    "    lemmatized_words_for_counting = [normalize_text(get_lemma(word)) for word in words_for_counting if len(word) >= 3]\n",
    "    \n",
    "    most_common_words = Counter(lemmatized_words_for_counting).most_common(100)\n",
    "    df_top_words = pd.DataFrame(most_common_words, columns=[\"Kelime (Lemma)\", \"Frekans\"])\n",
    "    with pd.ExcelWriter(OUTPUT_PATH, engine=\"openpyxl\", mode=\"a\") as writer:\n",
    "        df_top_words.to_excel(writer, index=False, sheet_name=\"Nötr En Çok Kelimeler (Lemma)\")\n",
    "else:\n",
    "    print(\"⚠️ No neutral reviews found or aspect columns missing for neutral word analysis.\")\n",
    "\n",
    "print(f\"Process Done✅: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
